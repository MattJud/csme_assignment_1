{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "In this exercise you will implement a simple network with pytorch. The network will have the same architecture as the one in exercise 1:\n",
    "input - fully connected layer - ReLU - fully connected layer - softmax\n",
    "We will test the network with real data and you will tune in the hyperparameters of the network to achieve high accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get MNIST data\n",
    "Download the MNIST dataset and set the parameters 'location_images' and 'location_labels' to the location of your downloaded files. \n",
    "(You can check out this website http://rasbt.github.io/mlxtend/user_guide/data/loadlocal_mnist/ for help. We recomend downloading the 10k files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to the path on your device\n",
    "location_images = 't10k-images-idx3-ubyte'\n",
    "location_labels = 't10k-labels-idx1-ubyte'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the location is correct we can import the data.\n",
    "Note: Make sure mlxtend is installed on your device!\n",
    "(If you are using anaconda you can find installation help here https://anaconda.org/conda-forge/mlxtend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the data X: 10000 x 784\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.data import loadlocal_mnist\n",
    "\n",
    "X, y = loadlocal_mnist(images_path=location_images, labels_path=location_labels)\n",
    "\n",
    "print('Dimension of the data X: %s x %s' % (X.shape[0], X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 784])\n"
     ]
    }
   ],
   "source": [
    "# This code initializes a mini batch (X_batch and y_batch) of your downloaded data\n",
    "# as well as the weights of the network. \n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def get_data_batch(X, y, batch_size, seed):\n",
    "    np.random.seed(seed)\n",
    "    num_train = X.shape[0]\n",
    "    rand_indices = np.random.choice(np.arange(num_train), size=batch_size, replace=True)\n",
    "    X_batch = torch.FloatTensor(X[rand_indices,:]).type(torch.FloatTensor)\n",
    "    y_batch = torch.LongTensor(y[rand_indices]).type(torch.LongTensor)\n",
    "    return X_batch, y_batch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "batch_size = 1000\n",
    "input_dim = X.shape[1]\n",
    "hidden_dim = 10\n",
    "num_classes = 10\n",
    "X_batch, y_batch = get_data_batch(X, y, batch_size, 0)\n",
    "\n",
    "print(X_batch.size())\n",
    "\n",
    "# Pay attention to the requires_grad=True statements these will be usefull later!\n",
    "w1 = torch.randn(input_dim, hidden_dim, requires_grad=True)\n",
    "w2 = torch.randn(hidden_dim, num_classes, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax\n",
    "As in exercise 1 we want to calculate the loss with softmax. Pytorch has built in functions to calculate the softmax function efficiently. Please implement the 'SoftmaxLoss' function below using pytorch. Return the loss with the parameter outputs. You will have to overwrite the parameter in the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "def SoftmaxLoss(outputs, labels):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    outputs: a NxM tensor of type FloatTensor\n",
    "    labels: a N shape tensor of type LongTensor\n",
    "    \n",
    "    Return:\n",
    "    outputs: the softmax loss. \n",
    "    \"\"\"\n",
    "    batch_size = outputs.size()[0]\n",
    "    \n",
    "    ######################################## START OF YOUR CODE ########################################\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    outputs = criterion(outputs, labels)\n",
    "    ######################################## END OF YOUR CODE ##########################################\n",
    "    \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your loss:\n",
      "tensor(2.7732)\n",
      "\n",
      "correct loss:\n",
      "2.7732\n",
      "\n",
      "Difference between your loss and correct loss:\n",
      "tensor(-2.2888e-05)\n"
     ]
    }
   ],
   "source": [
    "# Test your implementation of the softmax function\n",
    "torch.manual_seed(1)\n",
    "\n",
    "test_out = torch.randn(200,10)\n",
    "test_lab = torch.LongTensor(200).random_(0, 10)\n",
    "test_loss = SoftmaxLoss(test_out, test_lab)\n",
    "print('Your loss:')\n",
    "print(test_loss)\n",
    "print()\n",
    "print('correct loss:')\n",
    "correct_loss = 2.7732\n",
    "print(correct_loss)\n",
    "print()\n",
    "\n",
    "# The difference should be small. We get < 1e-4\n",
    "print('Difference between your loss and correct loss:')\n",
    "print(test_loss - correct_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the network\n",
    "With pytorch building and training neural networks gets a lot simpler. Below the forward propagation and the loss function are implemented for you. Implement the missing back propagation below (Hint: you will only need one line of code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 144.743\n",
      "[1,   100] loss: 131.756\n",
      "[1,   150] loss: 121.287\n",
      "[1,   200] loss: 112.820\n",
      "[1,   250] loss: 105.722\n",
      "[1,   300] loss: 99.789\n",
      "[1,   350] loss: 94.773\n",
      "[1,   400] loss: 90.405\n",
      "[1,   450] loss: 86.619\n",
      "[1,   500] loss: 83.328\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6\n",
    "iterations = 500\n",
    "for t in range(iterations):\n",
    "    \n",
    "    #--------------------------------------- forward propagation ---------------------------------------\n",
    "\n",
    "    y_pred = X_batch.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "\n",
    "    #--------------------------------------- loss function ---------------------------------------------\n",
    "    \n",
    "    loss = SoftmaxLoss(y_pred, y_batch)\n",
    "    \n",
    "    \n",
    "    #--------------------------------------- back propagation -------------------------------------------\n",
    "    \n",
    "    ######################################## START OF YOUR CODE ########################################\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    ######################################## END OF YOUR CODE ##########################################\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximise the accuracy\n",
    "With the code provided below try to achieve the highest possible accuracy. To do this tune in the 'learning_rate', 'iterations' and 'batch_size' of the training and validation set. You might want to implement code above to loop through different learning_rates and save your best accuracies. Note: There is no START OF YOUR CODE and END OF YOUR CODE line in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4163.3174, grad_fn=<NllLossBackward>)\n",
      "tensor(4414.1660, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Get Validation set\n",
    "X_val, y_val = get_data_batch(X, y, batch_size, 100)\n",
    "\n",
    "# Make a prediction with the trained network on the training and validation batch\n",
    "\n",
    "y_pred_batch = X_batch.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "y_pred_val = X_val.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "loss_train = SoftmaxLoss(y_pred_batch, y_batch)\n",
    "loss_val = SoftmaxLoss(y_pred_val, y_val)\n",
    "\n",
    "print(loss_train)\n",
    "print(loss_val)\n",
    "\n",
    "# compute the accuracy\n",
    "\n",
    "# print('Training accuracy: ', torch.mean(compare_train.float()))\n",
    "# print('Validation accuracy: ', torch.mean(compare_val.float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 38.927\n",
      "[1,   200] loss: 36.707\n",
      "[1,   300] loss: 34.748\n",
      "[1,   400] loss: 33.034\n",
      "[1,   500] loss: 31.503\n",
      "\n",
      "\n",
      "[2,   100] loss: 28.979\n",
      "[2,   200] loss: 26.847\n",
      "[2,   300] loss: 25.043\n",
      "[2,   400] loss: 23.584\n",
      "[2,   500] loss: 22.314\n",
      "\n",
      "\n",
      "[3,   100] loss: 20.639\n",
      "[3,   200] loss: 19.162\n",
      "[3,   300] loss: 17.821\n",
      "[3,   400] loss: 16.569\n",
      "[3,   500] loss: 15.449\n",
      "\n",
      "\n",
      "[4,   100] loss: 14.124\n",
      "[4,   200] loss: 12.949\n",
      "[4,   300] loss: 11.876\n",
      "[4,   400] loss: 10.884\n",
      "[4,   500] loss: 9.971\n",
      "\n",
      "\n",
      "[5,   100] loss: 8.929\n",
      "[5,   200] loss: 8.047\n",
      "[5,   300] loss: 7.279\n",
      "[5,   400] loss: 6.590\n",
      "[5,   500] loss: 5.983\n",
      "\n",
      "\n",
      "[6,   100] loss: 5.342\n",
      "[6,   200] loss: 4.785\n",
      "[6,   300] loss: 4.288\n",
      "[6,   400] loss: 3.845\n",
      "[6,   500] loss: 3.455\n",
      "\n",
      "\n",
      "[7,   100] loss: 3.056\n",
      "[7,   200] loss: 2.718\n",
      "[7,   300] loss: 2.434\n",
      "[7,   400] loss: 2.199\n",
      "[7,   500] loss: 1.999\n",
      "\n",
      "\n",
      "[8,   100] loss: 1.804\n",
      "[8,   200] loss: 1.642\n",
      "[8,   300] loss: 1.508\n",
      "[8,   400] loss: 1.401\n",
      "[8,   500] loss: 1.309\n",
      "\n",
      "\n",
      "[9,   100] loss: 1.214\n",
      "[9,   200] loss: 1.128\n",
      "[9,   300] loss: 1.049\n",
      "[9,   400] loss: 0.978\n",
      "[9,   500] loss: 0.914\n",
      "\n",
      "\n",
      "[10,   100] loss: 0.852\n",
      "[10,   200] loss: 0.797\n",
      "[10,   300] loss: 0.755\n",
      "[10,   400] loss: 0.718\n",
      "[10,   500] loss: 0.685\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iterations = 500\n",
    "epoch = 0\n",
    "for epoch in range (10):\n",
    "    \n",
    "    learning_rate = 1e-6 * (epoch+1)\n",
    "    \n",
    "    w1.grad.zero_()\n",
    "    w2.grad.zero_()\n",
    "    \n",
    "    for t in range(iterations):\n",
    "    \n",
    "        running_loss = 0.0\n",
    "\n",
    "        y_pred = X_batch.mm(w1).clamp(min=0).mm(w2)\n",
    "   \n",
    "        loss = SoftmaxLoss(y_pred, y_batch)\n",
    " \n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            w1 -= learning_rate * w1.grad\n",
    "            w2 -= learning_rate * w2.grad\n",
    "            w1.grad.zero_()\n",
    "            w2.grad.zero_()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if t % 100 == 99:    # print every 50 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, t + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was the highest accuracy you achieved and with which value of the 'learning_rate', 'iterations' and 'batch_size' did you achieve this accuracy? Write down your answere below and shortly describe the procedure you used to find them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
